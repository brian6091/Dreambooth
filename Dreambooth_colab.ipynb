{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dreambooth-style fine tuning of Stable Diffusion models\n",
        "\n",
        "Yet another notebook for training Stable Diffusion models.\n",
        "\n",
        "Tested with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) and [Stable Diffusion v2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base).\n",
        "\n",
        "Stay up-to-date on [Github](https://github.com/brian6091/Dreambooth), and leave an [issue](https://github.com/brian6091/Dreambooth/issues) if you run into problems.\n",
        "\n",
        "[![Brian6091's GitHub stats](https://github-readme-stats.vercel.app/api?username=brian6091&hide=contribs&theme=onedark&show_icons=true)](https://github.com/brian6091/Dreambooth)\n",
        "\n",
        "[<a href=\"https://www.buymeacoffee.com/jvsurfsqv\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" height=\"40px\" width=\"140px\" alt=\"Buy Me A Coffee\"></a>](https://www.buymeacoffee.com/jvsurfsqv)"
      ],
      "metadata": {
        "id": "EDwM5xLRggN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with the default Colab python==3.8, diffusers==0.9.0 and accelerate==0.14.0. \n",
        "\n",
        "Using accelerate==0.15.0 and diffusers==0.10.0dev results in casting errors.\n",
        "\n",
        "TODO:\n",
        "* Using different text encoders\n",
        "* Test at 768 resolution"
      ],
      "metadata": {
        "id": "Npk7tvLmPTMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies (takes about 1 minute)"
      ],
      "metadata": {
        "id": "LouTFfVYhRei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd /content/\n",
        "!git clone -b drop https://github.com/brian6091/Dreambooth\n",
        "!pip install -r \"Dreambooth/requirements.txt\"\n",
        "!pip install -U --pre triton"
      ],
      "metadata": {
        "id": "PmsR_IPcvp7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title xformers\n",
        "%%capture\n",
        "!nvidia-smi\n",
        "\n",
        "# Tested with Tesla T4 and A100 GPUs\n",
        "!pip install https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "7tfenRTEQz_R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which model to train from?"
      ],
      "metadata": {
        "id": "8C3LrrpBvYn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YI6dHfQ8iqMg"
      },
      "outputs": [],
      "source": [
        "#@title Name or path to initial model and VAE\n",
        "#@markdown Obligatory (e.g., runwayml/stable-diffusion-v1-5 or stabilityai/stable-diffusion-2-base)\n",
        "MODEL_NAME_OR_PATH = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Optional (e.g., stabilityai/sd-vae-ft-mse), leaving empty will default to VAE packaged with the model\n",
        "VAE_NAME_OR_PATH = \"stabilityai/sd-vae-ft-mse\" #@param {type:\"string\"}\n",
        "#if VAE_NAME_OR_PATH==\"\":\n",
        "#  VAE_NAME_OR_PATH = None\n",
        "\n",
        "#@markdown (Not yet implemented), leaving empty will default to text encoder packaged with the model\n",
        "TEXT_ENCODER_NAME_OR_PATH = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hugging Face ðŸ¤— credentials\n",
        "\n",
        "#@markdown If initiating training from official stable diffusion checkpoints (e.g., [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)), you must accept the license before using the model. You'll need a [ðŸ¤— Hugging Face](https://huggingface.co/) account to do so, after which you can [generate a login token](https://huggingface.co/settings/tokens) and paste it here.\n",
        "from huggingface_hub import login\n",
        "\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "login(HUGGINGFACE_TOKEN)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BGzA0N0C0pDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive if initial model stored there (or you want to direct outputs there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "2wBnGW_v00va",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up experiment parameters"
      ],
      "metadata": {
        "id": "fxP_d_n4mW2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1cJJ-P8jPhx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "#@title Training parameters\n",
        "\n",
        "#@markdown Unique token for specific subject\n",
        "INSTANCE_TOKEN= \"raretoken\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown Are instance image filenames captioned?\n",
        "CAPTIONED_INSTANCE_IMAGES = False #@param {type:\"boolean\"}\n",
        "CAPTIONED_INSTANCE_IMAGES_FLAG = \"\"\n",
        "if CAPTIONED_INSTANCE_IMAGES:\n",
        "  CAPTIONED_INSTANCE_IMAGES_FLAG='--image_captions_filename'\n",
        "\n",
        "#@markdown Path to instance images. Filenames are irrelevant, unless images are captioned, in which case INSTANCE_TOKEN should appear in relevant filenames as part of the caption.\n",
        "INSTANCE_DIR=\"/content/gdrive/MyDrive/InstanceImages/\" #@param{type: 'string'}\n",
        "\n",
        "RESOLUTION = 512 #@param{type: 'number'}\n",
        "\n",
        "TRAIN_BATCH_SIZE = 4 #@param{type: 'number'}\n",
        "\n",
        "GRADIENT_ACCUMULATION_STEPS = 1  #@param{type: 'number'}\n",
        "\n",
        "GRADIENT_CHECKPOINTING = True #@param {type:\"boolean\"}\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "  GRADIENT_CHECKPOINTING_FLAG='--gradient_checkpointing'\n",
        "\n",
        "ENABLE_PRIOR_PRESERVATION = True #@param {type:\"boolean\"} \n",
        "if ENABLE_PRIOR_PRESERVATION:\n",
        "  ENABLE_PRIOR_PRESERVATION_FLAG='--with_prior_preservation'\n",
        "\n",
        "#@markdown Prior loss weight. Note that if you set this to 0, but enable prior preservation and provide a CLASS_DIR, you can monitor class loss.\n",
        "PRIOR_LOSS_WEIGHT = 1.0 #@param {type:\"number\"} \n",
        "\n",
        "#@markdown If using prior preservation, specify a path to class images\n",
        "CLASS_DIR=\"/content/gdrive/MyDrive/RegularizationImages/person/\" #@param{type: 'string'}\n",
        "if (CLASS_DIR !=\"\") and os.path.exists(str(CLASS_DIR)):\n",
        "  CLASS_DIR=CLASS_DIR\n",
        "elif (CLASS_DIR !=\"\") and not os.path.exists(str(CLASS_DIR)):\n",
        "  CLASS_DIR=input('\u001b[1;31mThe folder specified does not exist, use the colab file explorer to copy the path :')\n",
        "\n",
        "#@markdown Prompt for prior preservation class (e.g., 'person', 'a photo of a man', 'dog')\n",
        "CLASS_PROMPT=\"a photo of a person\" #@param {type:\"string\"}\n",
        "#@markdown Instance prompt, {SKS} will be automatically replaced by INSTANCE_TOKEN defined above\n",
        "INSTANCE_PROMPT=\"a photo of {SKS} person\" #@param {type:\"string\"}\n",
        "INSTANCE_PROMPT=INSTANCE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
        "\n",
        "#@markdown Specify the number of class images used if prior preservation is enabled. If there are not enough images in CLASS_DIR (or CLASS_DIR is empty), additional images will be generated.\n",
        "MIN_NUM_CLASS_IMAGES=1500 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Batch size for generating class images \n",
        "SAMPLE_BATCH_SIZE = 4 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Number of training iterations, e.g., # instance images * 100\n",
        "STEPS = 399 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Random number generator seed\n",
        "SEED = 1275017 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Enable text encoder training?\n",
        "TRAIN_TEXT_ENCODER = True #@param{type: 'boolean'}\n",
        "if TRAIN_TEXT_ENCODER:\n",
        "  TRAIN_TEXT_ENCODER_FLAG=\"--train_text_encoder\"\n",
        "else:\n",
        "  TRAIN_TEXT_ENCODER_FLAG=\"\"\n",
        "\n",
        "#@markdown ## ADAM optimizer settings\n",
        "\n",
        "#@markdown Use 8-bit ADAM\n",
        "USE_8BIT_ADAM = True #@param {type:\"boolean\"} \n",
        "if USE_8BIT_ADAM:\n",
        "  USE_8BIT_ADAM_FLAG='--use_8bit_adam'\n",
        "else:\n",
        "  USE_8BIT_ADAM_FLAG=\"\"\n",
        "\n",
        "#@markdown The exponential decay rate for the 1st moment estimates (the beta1 parameter for the Adam optimizer).\n",
        "ADAM_BETA1 = 0.9 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown The exponential decay rate for the 2nd moment estimates (the beta2 parameter for the Adam optimizer).\n",
        "ADAM_BETA2 = 0.999 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Weight decay magnitude for the Adam optimizer.\n",
        "ADAM_WEIGHT_DECAY = 1e-2 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Epsilon value for the Adam optimizer.\n",
        "ADAM_EPSILON = 1e-08 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown \"fp16\", \"bf16\", or \"no\" according to available VRAM\n",
        "MIXED_PRECISION = \"fp16\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown ## Learning rate parameters\n",
        "LR_SCHEDULE = \"cosine\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "LR = 0.000005 #@param{type: 'number'}\n",
        "LR_WARMUP_STEPS = 25 #@param{type: 'number'}\n",
        "#@markdown Applies only for cosine_with_restarts schedule\n",
        "LR_COSINE_NUM_CYCLES = 5 #@param{type: 'number'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Experimental training parameters\n",
        "\n",
        "#@markdown ## [Drop text-conditioning to improve classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)\n",
        "\n",
        "#@markdown Probability that image (applies to both instance and class images) will be selected for dropout (INSTANCE_PROMPT/CLASS_PROMPT will be replaced with UNCONDITIONAL_PROMPT)\n",
        "CONDITIONING_DROPOUT_PROB = 0.0 #@param{type: 'number'}\n",
        "#@markdown Defaults to an empty prompt. Unsure whether anything else would be useful.\n",
        "UNCONDITIONAL_PROMPT = \" \" #@param{type: 'string'}\n",
        "\n",
        "#@markdown ## Exponentially-weight moving average weights (unet only). Will not run on Tesla T4 (out of memory).\n",
        "USE_EMA = False #@param{type: 'boolean'}\n",
        "if USE_EMA:\n",
        "  USE_EMA_FLAG=\"--use_ema\"\n",
        "else:\n",
        "  USE_EMA_FLAG=\"\"\n",
        "EMA_INV_GAMMA = 1.0 #@param{type: 'number'}\n",
        "EMA_POWER = 0.75 #@param{type: 'number'}\n",
        "EMA_MIN_VALUE = 0 #@param{type: 'number'}\n",
        "EMA_MAX_VALUE = 0.9999 #@param{type: 'number'}"
      ],
      "metadata": {
        "id": "LU7NC1Pkr47k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Where should outputs get saved?\n",
        "\n",
        "#@markdown Trained models (and intermediates) saved here\n",
        "OUTPUT_DIR=\"/content/models/\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown Training logs saved here\n",
        "LOGGING_DIR=\"/content/logs/\" #@param{type: 'string'}\n",
        "\n",
        "if not os.path.exists(LOGGING_DIR):\n",
        "  !mkdir -p \"$LOGGING_DIR\"\n",
        "\n",
        "LOG_GPU = False #@param{type: 'boolean'}\n",
        "if LOG_GPU:\n",
        "  LOG_GPU_FLAG=\"--log_gpu\"\n",
        "else:\n",
        "  LOG_GPU_FLAG=\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Lji3GATOYIg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup saving of intermediate models\n",
        "#@markdown To save intermediate checkpoints, set START_SAVING_FROM_STEP < STEPS\n",
        "\n",
        "#@markdown Number of steps between intermediate saves\n",
        "SAVE_CHECKPOINT_EVERY = 100 #@param{type: 'number'}\n",
        "if SAVE_CHECKPOINT_EVERY==None:\n",
        "  SAVE_CHECKPOINT_EVERY = STEPS+1\n",
        "\n",
        "START_SAVING_FROM_STEP=100 #@param{type: 'number'}\n",
        "if START_SAVING_FROM_STEP==None:\n",
        "  START_SAVING_FROM_STEP=STEPS\n",
        "\n",
        "#@markdown At each intermediate checkpoint, infer this many samples using SAVE_SAMPLE_PROMPT\n",
        "N_SAVE_SAMPLES=3 #@param{type: 'number'}\n",
        "\n",
        "#@markdown {SKS} is automatically replaced by INSTANCE_TOKEN. Give multiple prompts using // as a separator\n",
        "SAVE_SAMPLE_PROMPT= \"a photo of a person // a photo of {SKS} person // a photo of a cat\" #@param{type: 'string'}\n",
        "if SAVE_SAMPLE_PROMPT==\"\":\n",
        "  SAVE_SAMPLE_PROMPT=None\n",
        "else:\n",
        "  SAVE_SAMPLE_PROMPT=SAVE_SAMPLE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
        "\n",
        "#@markdown The negative prompt, on the other hand, applies to all SAVE_SAMPLE_PROMPTs\n",
        "SAVE_SAMPLE_NEGATIVE_PROMPT=\"hands, border\" #@param{type: 'string'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m9wXEuCnXn_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ],
      "metadata": {
        "id": "-sWFt9CCYkMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (optional) Tensorboard visualization of loss and learning rate\n",
        "#@markdown Once the Tensorboard panel is launched (takes a good 10 seconds), click on the gear icon in upper right, and check Reload data. Then, after launching training in the next cell, click on TIME SERIES in upper left to see updates.\n",
        "#%load_ext tensorboard\n",
        "!rm -rf /content/logs\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir $LOGGING_DIR "
      ],
      "metadata": {
        "id": "WfL1DJnZYr-S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch training\n",
        "!lsb_release -a | grep Description\n",
        "!pip freeze | grep diffusers\n",
        "!pip freeze | grep torchvision\n",
        "!pip freeze | grep transformers\n",
        "!pip freeze | grep xformers\n",
        "!accelerate env\n",
        "\n",
        "!accelerate launch \\\n",
        "    --mixed_precision=$MIXED_PRECISION \\\n",
        "    --num_machines=1 \\\n",
        "    --num_processes=1 \\\n",
        "    /content/Dreambooth/train_dreambooth.py \\\n",
        "    $CAPTIONED_INSTANCE_IMAGES_FLAG \\\n",
        "    $TRAIN_TEXT_ENCODER_FLAG \\\n",
        "    --revision=\"fp16\" \\\n",
        "    --pretrained_model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
        "    --pretrained_vae_name_or_path=$VAE_NAME_OR_PATH \\\n",
        "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
        "    --class_data_dir=\"$CLASS_DIR\" \\\n",
        "    --output_dir=\"$OUTPUT_DIR\" \\\n",
        "    --logging_dir=\"$LOGGING_DIR\" \\\n",
        "    $LOG_GPU_FLAG \\\n",
        "    $ENABLE_PRIOR_PRESERVATION_FLAG \\\n",
        "    --prior_loss_weight=$PRIOR_LOSS_WEIGHT \\\n",
        "    --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
        "    --class_prompt=\"$CLASS_PROMPT\" \\\n",
        "    --conditioning_dropout_prob=$CONDITIONING_DROPOUT_PROB \\\n",
        "    --unconditional_prompt=\"$UNCONDITIONAL_PROMPT\" \\\n",
        "    --seed=$SEED \\\n",
        "    --resolution=$RESOLUTION \\\n",
        "    --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
        "    --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
        "    $GRADIENT_CHECKPOINTING_FLAG \\\n",
        "    --mixed_precision=$MIXED_PRECISION \\\n",
        "    $USE_8BIT_ADAM_FLAG \\\n",
        "    --adam_beta1=$ADAM_BETA1 \\\n",
        "    --adam_beta2=$ADAM_BETA2 \\\n",
        "    --adam_weight_decay=$ADAM_WEIGHT_DECAY \\\n",
        "    --adam_epsilon=$ADAM_EPSILON \\\n",
        "    --learning_rate=$LR \\\n",
        "    --lr_scheduler=$LR_SCHEDULE \\\n",
        "    --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
        "    --lr_cosine_num_cycles=$LR_COSINE_NUM_CYCLES \\\n",
        "    $USE_EMA_FLAG \\\n",
        "    --ema_inv_gamma=$EMA_INV_GAMMA \\\n",
        "    --ema_power=$EMA_POWER \\\n",
        "    --ema_min_value=$EMA_MIN_VALUE \\\n",
        "    --ema_max_value=$EMA_MAX_VALUE \\\n",
        "    --max_train_steps=$STEPS \\\n",
        "    --num_class_images=$MIN_NUM_CLASS_IMAGES \\\n",
        "    --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
        "    --save_min_steps=$START_SAVING_FROM_STEP \\\n",
        "    --save_interval=$SAVE_CHECKPOINT_EVERY \\\n",
        "    --n_save_sample=$N_SAVE_SAMPLES \\\n",
        "    --save_sample_prompt=\"$SAVE_SAMPLE_PROMPT\" \\\n",
        "    --save_sample_negative_prompt=\"$SAVE_SAMPLE_NEGATIVE_PROMPT\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Zim-xlhbYlej",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do inference with trained model(s)\n",
        "\n",
        "Cells in this section can be run to generate grids of images using the trained model(s). I find this useful for probing overtraining, concept bleeding, quality, etc."
      ],
      "metadata": {
        "id": "q9tudAPlex_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some imports and utility functions\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, StableDiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def get_pipeline(model_name_or_path, \n",
        "                 vae_name_or_path=None, \n",
        "                 text_encoder_name_or_path=None,\n",
        "                 feature_extractor_name_or_path=None,\n",
        "                 revision=\"fp16\"):\n",
        "    #scheduler = DPMSolverMultistepScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
        "    scheduler = DPMSolverMultistepScheduler(\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        prediction_type=\"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        lower_order_final=True,\n",
        "    )\n",
        "\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        custom_pipeline=\"lpw_stable_diffusion\",\n",
        "        safety_checker=None,\n",
        "        revision=revision,\n",
        "        scheduler=scheduler,\n",
        "        vae=AutoencoderKL.from_pretrained(\n",
        "            vae_name_or_path or model_name_or_path,\n",
        "            subfolder=None if vae_name_or_path else \"vae\",\n",
        "            revision=None if vae_name_or_path else revision,\n",
        "            torch_dtype=torch.float16,\n",
        "        ),\n",
        "        feature_extractor=feature_extractor_name_or_path,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    #https://github.com/huggingface/diffusers/issues/1552\n",
        "    #pipe.enable_attention_slicing()\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    return pipe\n",
        "\n",
        "def get_config(filename=None,\n",
        "               save_dir=None,\n",
        "               prompt=None, negative_prompt=None,\n",
        "               seeds=None,\n",
        "               num_samples=4,\n",
        "               width=512, height=512,\n",
        "               inference_steps=20,\n",
        "               guidance_scale=7.5,\n",
        "               ):\n",
        "    if filename==None:\n",
        "        num_prompts = len(prompt)\n",
        "        if seeds==None:\n",
        "            generator = torch.Generator(\"cuda\")\n",
        "            seeds = []\n",
        "            for i in range(num_samples):\n",
        "                seed = generator.seed()\n",
        "                seeds.append(seed)\n",
        "        else:\n",
        "            num_samples = len(seeds)\n",
        "\n",
        "        tag = ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "        config = {\n",
        "            \"tag\": tag,\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"num_prompts\": num_prompts, \n",
        "            \"num_samples\": num_samples, \n",
        "            \"seeds\": seeds,\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"inference_steps\": inference_steps,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(save_dir, \"config_\"+tag+\".json\"), \"w\") as outfile:\n",
        "            json.dump(config, outfile)\n",
        "    else:\n",
        "        f = open(filename)\n",
        "        config = json.load(f)\n",
        "    \n",
        "    return config\n",
        "\n",
        "def get_images(pipe, sample_config, device=\"cuda\"):\n",
        "    generator = torch.Generator(\"cuda\")\n",
        "    with torch.autocast(device):\n",
        "        num_cfg = len(sample_config['guidance_scale'])\n",
        "        # Loop in order to use defined seed for each image in a batch\n",
        "        all_images = []\n",
        "        for i in range(sample_config['num_samples']):\n",
        "        #for _ in sample_config['num_samples']:\n",
        "            for cfg in sample_config['guidance_scale']:\n",
        "                # Manually generate latent\n",
        "                seed = sample_config['seeds'][i]\n",
        "                generator = generator.manual_seed(seed)\n",
        "                latent = torch.randn(\n",
        "                    (1, pipe.unet.in_channels, sample_config['height'] // 8, sample_config['width'] // 8),\n",
        "                    generator = generator,\n",
        "                    device = device\n",
        "                )\n",
        "                images = pipe(sample_config['prompt'],\n",
        "                    negative_prompt=sample_config['negative_prompt'],\n",
        "                    num_inference_steps=int(sample_config['inference_steps']),\n",
        "                    guidance_scale=cfg,\n",
        "                    latents=latent.repeat(sample_config['num_prompts'], 1, 1, 1),\n",
        "                ).images\n",
        "                all_images.extend(images)\n",
        "\n",
        "    grid = image_grid(all_images, rows=num_cfg*sample_config['num_samples'], cols=sample_config['num_prompts'])\n",
        "    return grid"
      ],
      "metadata": {
        "id": "-3nj_hjwGFCf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify which models to do inference with\n",
        "model_list = [os.path.join(OUTPUT_DIR,'100'),\n",
        "              os.path.join(OUTPUT_DIR,'200'),\n",
        "              os.path.join(OUTPUT_DIR,'300'),\n",
        "              os.path.join(OUTPUT_DIR,'399'),\n",
        "              ]\n",
        "\n",
        "print(model_list)"
      ],
      "metadata": {
        "id": "w3Xv0Zks-fCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate or load a configuration for inference\n",
        "\n",
        "config_name = None\n",
        "#config_name = os.path.join(OUTPUT_DIR, \"config_ZMasiqkP.json\")\n",
        "\n",
        "if config_name is None:\n",
        "    num_samples = 8\n",
        "    prompt = [\"photo of a cat\",\n",
        "              \"photo of a person\",\n",
        "              \"close-up studio portrait photo of Keanu Reeves, film, detail, studio lighting\",\n",
        "              \"close-up studio portrait photo of Caterina Murino, film, detail, studio lighting\",\n",
        "              \"close-up studio portrait photo of {SKS} person, film, detail, studio lighting\",\n",
        "              \"beautiful white (marble:1.1) bust of {SKS} person, highly detailed\",\n",
        "              \"oil painting of {SKS} person on the beach\",\n",
        "              \"portrait of {SKS} person as an elf princess in Lord of the Rings, natural lighting, erthereal, outdoors, intricate, highly detailed, digital painting, artstation, concept art, DeviantArt, illustration, art by artgerm and greg rutkowski and alphonse mucha\",\n",
        "    ]\n",
        "    negative_prompt = \"hands, nude, nudity, duplicate, frame, border\"\n",
        "    guidance_scale = [1.0, 3.0, 7.0, 15.0]\n",
        "\n",
        "    config = get_config(save_dir=OUTPUT_DIR,\n",
        "                        prompt=prompt, negative_prompt=negative_prompt,\n",
        "                        num_samples=num_samples,\n",
        "                        width=512, height=512, \n",
        "                        inference_steps=20, guidance_scale=guidance_scale\n",
        "                        )\n",
        "else:\n",
        "    config = get_config(filename=config_name)\n",
        "\n",
        "config['prompt'] = [sub.replace('{SKS}', INSTANCE_TOKEN) for sub in config['prompt']]\n",
        "print(config)"
      ],
      "metadata": {
        "id": "ub3iLhz0aBho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Infer!\n",
        "for model in model_list:\n",
        "    print(model)\n",
        "    pipe = get_pipeline(model)\n",
        "    grid = get_images(pipe, config)\n",
        "    #grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model)[1]+\"_\"+config['tag']+\".png\"))\n",
        "    grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model)[1]+\"_\"+config['tag']+\".jpg\"), quality=90, optimize=True)\n",
        "    del pipe\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ZMMHcQ55_PAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate grids for base model using same config\n",
        "model_name_or_path = MODEL_NAME_OR_PATH #'runwayml/stable-diffusion-v1-5'\n",
        "vae_name_or_path = VAE_NAME_OR_PATH #'stabilityai/sd-vae-ft-mse'\n",
        "pipe = get_pipeline(model_name_or_path, vae_name_or_path=vae_name_or_path)\n",
        "grid = get_images(pipe, config)\n",
        "#grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model_name_or_path)[1]+\"_\"+config['tag']+\".png\"))\n",
        "grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model_name_or_path)[1]+\"_\"+config['tag']+\".jpg\"), quality=90, optimize=True)\n",
        "\n",
        "del pipe\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Bq4VAh6EPhja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to checkpoint (ckpt) or safetensors format"
      ],
      "metadata": {
        "id": "W2WxeLuIqW2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "\n",
        "MODEL_PATH = os.path.join(OUTPUT_DIR, '500')\n",
        "CKPT_PATH = os.path.join(OUTPUT_DIR, '500.ckpt')\n",
        "\n",
        "!python /content/convert_diffusers_to_original_stable_diffusion.py \\\n",
        "  --model_path $MODEL_PATH \\\n",
        "  --checkpoint_path $CKPT_PATH \\\n",
        "  --half"
      ],
      "metadata": {
        "id": "hX2OSGlOuDD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Close Colab instance"
      ],
      "metadata": {
        "id": "_fE3xIr7lE2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "HYrrT17slGd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LouTFfVYhRei"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}